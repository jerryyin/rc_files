---
globs: **/triton*/**
alwaysApply: false
---
# Triton Kernel Development Rules

Guidelines for writing correct and performant Triton kernels.

## Tile Size Constraints

- **Always use power-of-2** for `BLOCK_SIZE`, `BLOCK_M`, `BLOCK_N`, `BLOCK_K`
- Tile dimensions in `tl.arange(start, end)` must be compile-time constants (`tl.constexpr`)
- Non-power-of-2 tile sizes will fail compilation

## Masking Rules

**Always mask at boundaries:**
```python
offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
mask = offsets < n_elements  # REQUIRED when BLOCK_SIZE may not divide n_elements
data = tl.load(ptr + offsets, mask=mask, other=0.0)
tl.store(ptr + offsets, result, mask=mask)
```

## Anti-Patterns to Avoid

❌ **Scalar loops over elements:**
```python
for i in range(BLOCK_SIZE):
    x = tl.load(ptr + i)  # BAD: one element at a time
```
✅ Use vectorized loads:
```python
offsets = tl.arange(0, BLOCK_SIZE)
x = tl.load(ptr + offsets)  # GOOD: vectorized
```

❌ **Runtime values in tl.arange():**
```python
offsets = tl.arange(0, N)  # BAD if N is not constexpr
```
✅ Use constexpr parameters:
```python
def kernel(ptr, N, BLOCK: tl.constexpr):
    offsets = tl.arange(0, BLOCK)  # GOOD
```

❌ **Accumulating in low precision:**
```python
acc = tl.zeros((M, N), dtype=tl.float16)  # BAD: precision loss
```
✅ Accumulate in float32:
```python
acc = tl.zeros((M, N), dtype=tl.float32)  # GOOD
acc += tl.dot(a.to(tl.float16), b.to(tl.float16))
```

## Type Conversion Best Practices

- **FP8 inputs → FP16/FP32 accumulation → output type**
- Use explicit `.to()` for type conversions
- Match load types to storage format
- Avoid unnecessary back-and-forth conversions

## num_warps vs CUDA blockDim

`num_warps` is NOT like CUDA `blockDim`. It controls parallelism within tile operations.
The compiler maps tile operations to warps. Don't confuse with iteration space.

## Performance Quick Checks

1. **Register spills:** Look for `scratch_load`/`scratch_store` in assembly
2. **Layout conversions in loops:** Check TTGIR for `convert_layout` in hot paths
3. **Memory coalescing:** Adjacent threads should access adjacent memory